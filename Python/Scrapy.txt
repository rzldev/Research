## Web Scraping is the process of extracting data from websites

## First, it sends a “GET” query to a specific website. Then, it parses an HTML document based on the received result.
# After it’s done, the scraper searches for the data you need within the document, and, finally, converts it into the specified format.

## Robots.txt is rule for crawling data on website
## Scrapy will automatically follow all those rules

## Tutorial create a project with scrapy
# 1. Open your IDE(PyCharm for recomended)
# 2. Create a new project or a new virtualenv
# 3. Install Scrapy to your virtualenv
# source virtualenv/bin/activate
# pip install scrapy
# 4. To create Scrapy project type this command into terminal
# scrapy startproject myproject [project_dir]

## Spiders are classes which define how a certain site will be scraped, 
# including how to perform the crawl and how to extract structured data from their pages.

## settings.py
# BOT_NAME = The name of the bot implemented by this Scrapy project.
# USER_AGENT = The default User-Agent to use when crawling, unless overridden. BOT_NAME will be used to construct a USER_AGENT
# CONCURRENT_REQUESTS = The maximum number of concurrent requests that will be performed by the Scrapy downloader.

## items.py
# The main goal in scraping is to extract structured data from unstructured sources, typically, web pages

## middlewares.py
# framework of hooks into Scrapy’s spider processing mechanism 
# where you can plug custom functionality to process the responses that are sent to Spiders 
# for processing and to process the requests and items that are generated from spiders.

## Create a spider(Web Crawler)
# 1. Create a python file inside spiders directory
# 2. Create a name variable and start_url variable
# 3. Create a parse method
# 4. Inside parse method we can grab items that we want from web html by create a variable and its syntax
# 5. End method with yield

## Run scrapy spider
# 1. Instal ApiWin32 (for Windows)
# pip install pywin32
# 2. Run scrapy spider with its name
# scrapy crawl [name]

## Scraping data with CSS Selectors
# scrapy shell "[site name]"

## Storing data into JSON, XML, or, CSV files
# scrapy crawl [name] -o [file name].json
# scrapy crawl [name] -o [file name].xml
# scrapy crawl [name] -o [file name].csv

## Install SQLite3
# pip install pip install db-sqlite3

## Install mysql
# pip install mysql-connector-python

## Install MongoDB
# pip install pymongo

## Bypass Restrictions using User-Agent
# USER_AGENT is a identity of your browser when you try to surf on the internet
# Set USER_AGENT in settings.py
# USER_AGENT = "[find goolebot user agent on https://developers.whatismybrowser.com]

# Or you can install using pip
# pip install scrapy-user-agents
# And paste this code into settings.py
# DOWNLOADER_MIDDLEWARES = {
#     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
#     'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,
# }

## Bypass Restrictions using Proxies
# Install scrapy proxy pool
# pip install scrapy-proxy-pool
# Add this code into settings.py
# PROXY_POOL_ENABLED = True
# DOWNLOADER_MIDDLEWARES = {
    # ...
#     'scrapy_proxy_pool.middlewares.ProxyPoolMiddleware': 610,
#     'scrapy_proxy_pool.middlewares.BanDetectionMiddleware': 620,
    # ...
# }

### Using Splash for Javascript ###
## Docker.io
# Install docker.io
#  sudo apt-get install docker.io -y
# Start docker
#  sudo service dockker start
# Verify docker
#  sudo docker run hello-world
## Pull splash image from docker
#  sudo docker pull scrapinghub/splash
# Start the splash container
# sudo docker run -it -p 8050:8050 scrapinghub/splash

